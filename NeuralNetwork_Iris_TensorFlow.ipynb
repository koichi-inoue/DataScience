{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ex8kTKUJeQ"
      },
      "source": [
        "# Neural Network Sample\n",
        "TensorFlow と Keras による IRIS の分類"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFrmWm15UWIP"
      },
      "source": [
        "## 準備\n",
        "ライブラリとデータの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31xT7hBwTyeI"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-33WpT7VWc51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e9b43f3-3fd9-4cdf-bed5-5982dea58452"
      },
      "source": [
        "iris.data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh-6zPSPWpJ9"
      },
      "source": [
        "iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tehvvGuRW3J1"
      },
      "source": [
        "## データの分離\n",
        "学習用 80% 　テスト用 20%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMUm2qF9XFMb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split as split\n",
        "X_train, X_test, y_train, y_test = split(iris.data, iris.target, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmJrMGPIaOVv"
      },
      "source": [
        "## モデルの作成\n",
        "入力層 4：sepal_length \tsepal_width \tpetal_length \tpetal_width  \n",
        "隠れ層 32  \n",
        "出力層 3：setosa, versicolor,  virginica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjxLAe3LaRlH"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(Dense(units=32, input_shape=(4,), activation = 'relu'))\n",
        "model.add(Dense(units=3, activation = 'softmax'))\n",
        "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHaAysA7es-b"
      },
      "source": [
        "##機械学習\n",
        "epocs は学習回数で、ここでは120個（全体150個の80%）の  \n",
        "データの学習を100回繰り返し行っています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsJpfzBpe1Ig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3658
        },
        "outputId": "1b9899f5-531a-48a8-cf2b-9abaa502006b"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 1.0028 - acc: 0.5750\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 0s 182us/step - loss: 0.8790 - acc: 0.6667\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 0s 162us/step - loss: 0.8457 - acc: 0.6917\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 0s 183us/step - loss: 0.8195 - acc: 0.7750\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 0s 167us/step - loss: 0.7984 - acc: 0.7250\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 0s 163us/step - loss: 0.7781 - acc: 0.6667\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 0s 173us/step - loss: 0.7562 - acc: 0.7083\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 0s 164us/step - loss: 0.7373 - acc: 0.6750\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 0s 167us/step - loss: 0.7211 - acc: 0.7333\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 0s 140us/step - loss: 0.7055 - acc: 0.7333\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 0s 170us/step - loss: 0.6899 - acc: 0.7250\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 0s 189us/step - loss: 0.6776 - acc: 0.8083\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 0s 152us/step - loss: 0.6642 - acc: 0.7583\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 0s 155us/step - loss: 0.6676 - acc: 0.7417\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 0s 143us/step - loss: 0.6374 - acc: 0.8000\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 0s 155us/step - loss: 0.6248 - acc: 0.7917\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 0s 152us/step - loss: 0.6159 - acc: 0.8583\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 0s 171us/step - loss: 0.6041 - acc: 0.8250\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 0s 140us/step - loss: 0.5960 - acc: 0.8250\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 0s 157us/step - loss: 0.5854 - acc: 0.7583\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 0s 160us/step - loss: 0.5761 - acc: 0.8583\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 0s 147us/step - loss: 0.5681 - acc: 0.8667\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 0s 155us/step - loss: 0.5618 - acc: 0.8917\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 0s 147us/step - loss: 0.5542 - acc: 0.8750\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 0s 173us/step - loss: 0.5480 - acc: 0.8583\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 0s 176us/step - loss: 0.5402 - acc: 0.8917\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 0s 138us/step - loss: 0.5371 - acc: 0.8500\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 0s 138us/step - loss: 0.5319 - acc: 0.7833\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 0s 178us/step - loss: 0.5198 - acc: 0.8917\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 0s 164us/step - loss: 0.5229 - acc: 0.8167\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 0s 170us/step - loss: 0.5144 - acc: 0.8250\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 0s 160us/step - loss: 0.5059 - acc: 0.8667\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 0s 164us/step - loss: 0.4979 - acc: 0.8833\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 0s 152us/step - loss: 0.4913 - acc: 0.8333\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 0s 149us/step - loss: 0.4796 - acc: 0.9667\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 0s 152us/step - loss: 0.4759 - acc: 0.9167\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 0s 150us/step - loss: 0.4704 - acc: 0.9000\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 0s 174us/step - loss: 0.4652 - acc: 0.9000\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 0s 177us/step - loss: 0.4623 - acc: 0.8667\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 0s 175us/step - loss: 0.4553 - acc: 0.9583\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 0s 169us/step - loss: 0.4583 - acc: 0.8333\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 0s 164us/step - loss: 0.4461 - acc: 0.9500\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 0s 152us/step - loss: 0.4433 - acc: 0.8917\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 0s 158us/step - loss: 0.4518 - acc: 0.8833\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 0s 182us/step - loss: 0.4373 - acc: 0.9417\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 0s 208us/step - loss: 0.4649 - acc: 0.8417\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 0s 204us/step - loss: 0.4298 - acc: 0.9250\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 0s 179us/step - loss: 0.4279 - acc: 0.8917\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 0s 159us/step - loss: 0.4222 - acc: 0.9667\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 0s 205us/step - loss: 0.4193 - acc: 0.9000\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 0s 158us/step - loss: 0.4172 - acc: 0.9250\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 0s 129us/step - loss: 0.4130 - acc: 0.9667\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 0s 140us/step - loss: 0.4093 - acc: 0.9750\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 0s 197us/step - loss: 0.4090 - acc: 0.9000\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 0s 196us/step - loss: 0.4057 - acc: 0.9333\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 0s 161us/step - loss: 0.4075 - acc: 0.9167\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 0s 142us/step - loss: 0.3999 - acc: 0.9667\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 0s 158us/step - loss: 0.3953 - acc: 0.9083\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 0s 162us/step - loss: 0.3988 - acc: 0.9583\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 0s 153us/step - loss: 0.3950 - acc: 0.9417\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 0s 186us/step - loss: 0.3884 - acc: 0.9167\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 0s 132us/step - loss: 0.3916 - acc: 0.9500\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 0s 153us/step - loss: 0.3835 - acc: 0.9750\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 0s 169us/step - loss: 0.3863 - acc: 0.9583\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 0s 163us/step - loss: 0.3853 - acc: 0.9333\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 0s 156us/step - loss: 0.3746 - acc: 0.9500\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 0s 134us/step - loss: 0.3718 - acc: 0.9667\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 0s 154us/step - loss: 0.3745 - acc: 0.9167\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 0s 156us/step - loss: 0.3713 - acc: 0.9583\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 0s 148us/step - loss: 0.3817 - acc: 0.9083\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 0s 141us/step - loss: 0.3678 - acc: 0.9333\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 0s 149us/step - loss: 0.3627 - acc: 0.9333\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 0s 165us/step - loss: 0.3628 - acc: 0.9583\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 0s 139us/step - loss: 0.3605 - acc: 0.9583\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 0s 167us/step - loss: 0.3560 - acc: 0.9583\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 0s 145us/step - loss: 0.3590 - acc: 0.9583\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 0s 162us/step - loss: 0.3517 - acc: 0.9667\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 0s 135us/step - loss: 0.3494 - acc: 0.9500\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 0s 144us/step - loss: 0.3497 - acc: 0.9583\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 0s 146us/step - loss: 0.3454 - acc: 0.9750\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 0s 164us/step - loss: 0.3438 - acc: 0.9583\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 0s 178us/step - loss: 0.3462 - acc: 0.9500\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 0s 178us/step - loss: 0.3425 - acc: 0.9250\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 0s 227us/step - loss: 0.3414 - acc: 0.9417\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 0s 209us/step - loss: 0.3379 - acc: 0.9667\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 0s 150us/step - loss: 0.3363 - acc: 0.9333\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 0s 178us/step - loss: 0.3321 - acc: 0.9500\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 0s 197us/step - loss: 0.3363 - acc: 0.9750\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 0s 169us/step - loss: 0.3277 - acc: 0.9583\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 0s 172us/step - loss: 0.3289 - acc: 0.9667\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 0s 159us/step - loss: 0.3263 - acc: 0.9667\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 0s 174us/step - loss: 0.3240 - acc: 0.9667\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 0s 155us/step - loss: 0.3214 - acc: 0.9667\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 0s 174us/step - loss: 0.3218 - acc: 0.9583\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 0s 196us/step - loss: 0.3228 - acc: 0.9583\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 0s 172us/step - loss: 0.3194 - acc: 0.9500\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 0s 183us/step - loss: 0.3137 - acc: 0.9667\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 0s 183us/step - loss: 0.3137 - acc: 0.9750\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 0s 177us/step - loss: 0.3110 - acc: 0.9417\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 0s 178us/step - loss: 0.3105 - acc: 0.9750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4697555a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VojHp3afs8x"
      },
      "source": [
        "## テスト用のデータでモデルの評価\n",
        "以下のとおり、テストデータでは96.7%の正解率になることがわかります"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVwRX-08gCTu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e23d11b2-fdef-4f80-bc7f-7838512a5b82"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, batch_size = 1)\n",
        "print(\"正解率(accuracy)=\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30/30 [==============================] - 0s 4ms/step\n",
            "正解率(accuracy)= 0.9666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ql__tVHhMQ9"
      },
      "source": [
        "##任意のデータを分類させる\n",
        "事例として 4個の入力を、6.4,  2.7,  5.3,  1.9 として、正しく分類するかを試します。  \n",
        "ちなみにこれは virginica 種のデータです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEgnDewCiTTz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a0ccd07-6618-4bbc-8314-9633176f2b4c"
      },
      "source": [
        "import numpy as np\n",
        "x = np.array([[6.4, 2.7, 5.3, 1.9]])\n",
        "r = model.predict(x)\n",
        "\n",
        "r"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00543651, 0.2909139 , 0.7036496 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOvrgK5kkBHp"
      },
      "source": [
        "## 結果\n",
        "出力された３つの数字はそれぞれ、  \n",
        "  \n",
        "setosa種であるの確率：0.5%  \n",
        "versicolor種である確率：29.1%   \n",
        "virginica種である確率：70.4%\n",
        "  \n",
        "を意味します。  \n",
        "virginia種であることを正しく言い当てています。"
      ]
    }
  ]
}